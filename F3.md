# 🧩 **F3 – Model Lifecycle Loop (Functional Architecture)**

*Covers VS-4 (Model Engineering & Registry) + links to VS-1 (Gateway Integration) and VS-7 (Governance Feedback).*

Purpose → show how **models are discovered, evaluated, optimized, deployed, governed, and reused** across PromProm’s GenAI platform.

---

## 🟦 Canvas Specification

* **Orientation:** Landscape 16 × 9
* **Flow:** Circular / clockwise loop (“Discover → Evaluate → Optimize → Deploy → Monitor → Govern → back to Discover”)
* **Visual Shape:** large ring or figure-eight in the center with entry/exit branches.
* **Color Cues:**

  * Blue = model flow / activity
  * Grey = repositories / data stores
  * Green = automation / tools
  * Orange = governance feedback

---

## 🔹 Functional Zones (Clockwise Order)

### **1️⃣ Model Catalog & Discovery (Hub Entry)**

*(Top-left quadrant)*

* **Model Catalog Service** (MLflow + Postgres Registry)
  *“Central index of internal and vendor models with metadata and risk tags.”*
* **Search & Reuse API** → *“Agents and RAG Builder query catalog for existing models.”*
  Arrow → **Evaluation Lab**.

> **Deliverables:** VS-4 D1 (Central Catalog), VS-1 integration (API Gateway access)

---

### **2️⃣ Model Evaluation & Benchmark Lab**

*(Top-center segment)*

* **Benchmark Runner Service** (LangChain Eval + Python Harness)
  *“Executes predefined tasks against candidate models.”*
* **Metrics Collector (Grafana Dashboard)** → Latency, Accuracy, Cost, Toxicity.
* Arrow down to **Prompt Optimization Studio.**

> **Deliverables:** VS-4 D2 (Evaluation Framework), RFP §5.4.3 metrics reporting.

---

### **3️⃣ Prompt Optimization & Fine-Tuning Studio**

*(Right-hand quadrant)*

* **Prompt Studio App** (Streamlit UI + LangGraph Optimizer)
  *“Experiment with prompt variants, context lengths, temperature.”*
* **Auto Ranker / Scorer Module** → selects best variant by metric weighting.
* **Fine-Tuning Pipeline** (SageMaker / Bedrock Job) → creates new model version.
  Arrow → **Deployment Service.**

> **Deliverables:** VS-4 D3 (Prompt Optimization Environment), VS-4 D4 (Fine-Tuning Pipeline).

---

### **4️⃣ Model Deployment & Runtime Registration**

*(Bottom-right quadrant)*

* **Deployment Service** (AWS SageMaker / K8s Helm Controller)
  *“Publishes approved models to runtime environments.”*
* **Gateway Adapter Plugin** → *“Registers endpoint with AI Gateway for routing.”*
* **Version Tagging & Rollback Controller** → *“Immutable version IDs for audit.”*
  Arrow → **Monitoring & Risk Profiler.**

> **Deliverables:** VS-4 D5 (Deployment Automation), VS-1 link (Gateway integration).

---

### **5️⃣ Monitoring & Risk Profiling**

*(Bottom segment)*

* **Telemetry Collector** (Prometheus + FinOps metrics)
  *“Collects runtime stats from Gateway and Agents.”*
* **Risk Profiler Engine** (Python Rules + OPA hooks)
  *“Scores deployed models for drift, bias, cost thresholds.”*
* Upward orange arrow → **Governance Hub.**

> **Deliverables:** VS-4 D6 (Risk Profiler), VS-7 link (Responsible AI).

---

### **6️⃣ Governance & Policy Feedback**

*(Bottom-left quadrant)*

* **Governance Hub / OPA Repo** → policy changes and approval records.
* **Audit Trail Generator (Splunk / ServiceNow)** → automatic reports.
* **Policy Push Service** → updates rules for Gateway + Prompt Studio.
  Arrow → back to **Model Catalog Service** (Completes loop).

> **Deliverables:** VS-7 D1 (Audit & Compliance Loop), RFP §5.2.7 Responsible AI.

---

## 🔄 **Numbered Flow Steps (to label on arrows)**

1️⃣ Model submitted or discovered in Catalog.
2️⃣ Evaluation runs on benchmark suite.
3️⃣ Prompts optimized / model fine-tuned.
4️⃣ Approved model deployed and registered with Gateway.
5️⃣ Telemetry and risk metrics captured in AIOps plane.
6️⃣ Governance engine reviews and updates policies.
7️⃣ Updated policies flow back to Catalog and Gateway for future deployments.

---

## 🧠 Footer Line

> *Functional view covering VS-4 (Model Engineering & Registry) with Governance link: illustrates the continuous lifecycle of model discovery, evaluation, optimization, deployment, and policy feedback per RFP §5.4.3 and §5.2.7.*

---

### 🎨 Design Guidelines

* Draw a **clockwise ring** with six major segments (1–6).
* Center label: **“Continuous Model Lifecycle Loop.”**
* Use curved arrows for flow; orange arrow closing governance loop.
* Small icons inside each segment (e.g., registry db, test tubes, dashboard, shield).
* Bottom-right corner legend: *“Components Reused: MLflow Registry, LangChain Eval, Prompt Studio, SageMaker, Gateway Adapter, Prometheus, OPA Governance.”*

---

### ✅ Outcome

When you build this slide, you’ll show:

* Every **Model Lifecycle deliverable (VS-4 D1–D6)** plus governance links.
* End-to-end traceability to Component Architecture elements.
* A clear visual story of **how models evolve from idea → production → policy feedback → reuse** within PromProm’s GenAI ecosystem.
